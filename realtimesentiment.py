# -*- coding: utf-8 -*-
"""realtimesentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JgihMd-I47fpIXG_ZAvk-EZZXOCmfCrJ
"""

pip install gnews

pip install requests

import requests

def fetch_top_headlines(api_key, country='in', category='general', language='en', max_results=10):
    url = f'https://gnews.io/api/v4/top-headlines'
    params = {
        'country': country,
        'category': category,
        'lang': language,
        'max': max_results,
        'apikey': api_key
    }
    response = requests.get(url, params=params)
    return response.json()

# Replace 'YOUR_API_KEY' with your actual GNews API key
api_key = '2ef1f5123905ae6f327d09fd011d1318'
headlines = fetch_top_headlines(api_key)

if 'articles' in headlines:
    for article in headlines['articles']:
        print(f"Title: {article['title']}")
        print(f"Source: {article['source']['name']}")
        print(f"Published At: {article['publishedAt']}")
        print(f"URL: {article['url']}")
        print('-' * 80)
else:
    print("Failed to retrieve news headlines.")

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("NewsSentimentClassification") \
    .getOrCreate()

# If you saved the real-time news as CSV
df = spark.read.csv("news_headlines_labeled.csv", header=True, inferSchema=True)
df.show()

from pyspark.ml import Pipeline
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.ml.classification import LogisticRegression

# Step 1: Tokenize
tokenizer = Tokenizer(inputCol="headline", outputCol="words")

# Step 2: Remove stopwords
remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")

# Step 3: Convert to TF features
hashingTF = HashingTF(inputCol="filtered_words", outputCol="rawFeatures", numFeatures=1000)

# Step 4: Compute IDF
idf = IDF(inputCol="rawFeatures", outputCol="features")

# Step 5: Logistic Regression
lr = LogisticRegression(featuresCol="features", labelCol="label")

# Step 6: Build pipeline
pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])

# Split data into training and testing
train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)

# Train the pipeline
model = pipeline.fit(train_df)

# Predict on test set
predictions = model.transform(test_df)
predictions.select("headline", "label", "prediction").show(truncate=False)

pip install streamlit pandas plotly

import pandas as pd
from gnews import GNews
from textblob import TextBlob
import time

# Initialize GNews
news = GNews(language='en')

# Function to fetch news and predict
def fetch_and_predict(model):
    articles = news.get_top_news()[:20]
    headlines = []
    for article in articles:
        title = article['title']
        headlines.append((title,))

    # Convert to Spark DataFrame
    new_df = spark.createDataFrame(headlines, ["headline"])

    # Predict using trained ML pipeline
    predictions = model.transform(new_df)

    # Collect as Pandas DataFrame for visualization
    pdf = predictions.select("headline", "prediction").toPandas()
    return pdf